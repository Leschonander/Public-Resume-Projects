```{r}
library(tidyverse)
library(tidytext)
library(lubridate)

library(RSQLite)

library(topicmodels)
library(widyr)
library(irlba)

data(stop_words)
```

```{r}
con = dbConnect(SQLite(), dbname="BuzzFeedDB.db")

query <- dbSendQuery(con, "SELECT * FROM BuzzFeed_data")
df <- dbFetch(query, n = -1)
```

```{r}
df %>%
  unnest_tokens(word, title) %>%
  anti_join(stop_words) -> df_words

df_words %>%
  group_by(category) %>%
  count(word, sort = TRUE) -> df_words_counts
```

```{r}
BuzzFeed_dtm <- df_words_counts %>%
  cast_dtm(category, word, n)

BuzzFeed_dtm
```

Latent Dirichlet allocation 

```{r}
BuzzFeed_LDA <- LDA(BuzzFeed_dtm, k = 10, control = list(seed = 321)) # Started at 10 but we will do 5 in practice
BuzzFeed_LDA
```
```{r}
category_topics <- tidy(BuzzFeed_LDA, matrix = "beta")
category_topics
```

```{r}
top_terms <- category_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```

```{r fig.height = 7, fig.width = 14}
(top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() + theme_my_axios() + labs(
    x = "Term",
    y = "Beta",
    title = "BuzzFeed News Topic Modeling",
    subtitle = "Source: BuzzFeed",
    caption = "@LarsESchonander"
  ) -> BuzzFeedTopicModelChart)
```
```{r}
save_chart_map("BuzzFeedTopicModel", BuzzFeedTopicModelChart)
```

Text Emebeding | Word2Vec Part

```{r}
df %>%
  rowid_to_column(., "titleID") -> df
```


```{r Skipgrams}
skipgrams <- df %>%
  unnest_tokens(ngram, title, token = "ngrams", n = 8) %>%
  mutate(ngramID = row_number()) %>%
  unite(skipgramID, titleID, ngramID) %>%
  unnest_tokens(word, ngram)
```

```{r Skipgram Probabilities}
skipgrams_prob <- skipgrams %>%
  pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>% # Count's Pairs
  mutate(p = n / sum(n))
```

```{r Unigram Prob}
unigram_probs <- df %>%
  unnest_tokens(word, title) %>%
  count(word, sort = TRUE) %>%
  mutate(p = n / sum(n))
```

```{r Normalized Prob}
# Gave 20 filter but let's do 15...
normalized_prob <- skipgrams_prob %>%
  filter(n > 15) %>%
  rename(word1 = item1, word2 = item2) %>%
    left_join(unigram_probs %>%
                  select(word1 = word, p1 = p),
              by = "word1") %>%
    left_join(unigram_probs %>%
                  select(word2 = word, p2 = p),
              by = "word2") %>%
    mutate(p_together = p / p1 / p2)
```


```{r}
normalized_prob[2005:2010,]
```

```{r}
pmi_matrix <- normalized_prob %>%
  mutate(pmi = log10(p_together)) %>%
  cast_sparse(word1, word2, pmi)
```

```{r}
pmi_matrix@x[is.na(pmi_matrix@x)] <- 0
pmi_svd <- irlba(pmi_matrix, 256, maxit = 500)
word_vectors <- pmi_svd$u
rownames(word_vectors) <- rownames(pmi_matrix)
```

```{r}
search_synonyms <- function(word_vectors, selected_vector) {
  
  similarities <- word_vectors %*% selected_vector %>%
    tidy() %>%
    as_tibble() %>%
    rename(token = .rownames, 
           similarity = unrowname.x.)
  
  similarities %>%
    arrange(-similarity)
  
}
```

```{r}
search_synonyms(word_vectors, word_vectors["facebook",])
```

```{r The small SVG edition}
pmi_svd_for_chart <- irlba(pmi_matrix, 2, maxit = 500)
word_vectors_for_chart <- pmi_svd_for_chart$u

rownames(word_vectors_for_chart) <- rownames(pmi_matrix)
 
forplot <- as.data.frame(word_vectors_for_chart[200:300,])
forplot$word <- rownames(forplot)

ggplot(forplot, aes(x=V1, y=V2, label = word)) + 
  geom_text(aes(label=word), hjust = 0, vjust = 0) +
  theme_my_axios() + labs(
    x = "First Dimension Created by SVD",
    y = "Second Dimension Created by SVD"
  )
```

```{r}
Vecplot <- as.data.frame(word_vectors[600:700,])
Vecplot$word <- rownames(forplot)

(ggplot(Vecplot, aes(x=V1, y=V2, label = word)) + 
  geom_text(aes(label=word), hjust = 0, vjust = 0, color = "#ee3322") +
  theme_my_axios() + labs(
    x = "First Dimension Created by SVD",
    y = "Second Dimension Created by SVD",
    title = "BuzzFeed News SVD Word Vector Plot Snapshot",
    subtitle = "Source: BuzzFeed",
    caption = "@LarsESchonander"
  ) -> BuzzFeedVectorSnap)
```
```{r}
save_chart("BuzzFeedWordVec", BuzzFeedVectorSnap)
```

```{r}
(facebook <- search_synonyms(word_vectors, word_vectors["facebook",]))
```

```{r}
(google <- search_synonyms(word_vectors, word_vectors["google",]))
```

```{r}
(facebook %>%
    mutate(selected = "facebook") %>%
    bind_rows(google %>%
                  mutate(selected = "google")) %>%
    group_by(selected) %>%
    top_n(15, similarity) %>%
    ungroup %>%
    mutate(token = reorder(token, similarity)) %>%
    ggplot(aes(token, similarity, fill = selected)) +
    scale_fill_manual(values = c("facebook" = "#4267b2", "google" = "#DB4437")) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~selected, scales = "free") +
    coord_flip() +
    theme_my_axios() +
    labs(x = NULL, title = "What word Vectors are most similar to Facebook or Google?",
    subtitle = "Source: BuzzFeed",
    caption = "@LarsESchonander") -> facebook_google )
```
```{r}
save_chart("FaceBookGoogle", facebook_google)
```


